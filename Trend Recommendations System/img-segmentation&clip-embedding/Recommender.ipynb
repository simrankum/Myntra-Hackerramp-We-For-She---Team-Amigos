{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch==2.3.1+cpu torchvision==0.18.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: clip in c:\\users\\aryan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.0)\n",
      "Requirement already satisfied: ftfy in c:\\users\\aryan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from clip) (6.2.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\aryan\\appdata\\roaming\\python\\python311\\site-packages (from clip) (24.1)\n",
      "Requirement already satisfied: regex in c:\\users\\aryan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from clip) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aryan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from clip) (4.64.1)\n",
      "Requirement already satisfied: torch in c:\\users\\aryan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from clip) (2.4.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\aryan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from clip) (0.19.0)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in c:\\users\\aryan\\appdata\\roaming\\python\\python311\\site-packages (from ftfy->clip) (0.2.13)\n",
      "Requirement already satisfied: filelock in c:\\users\\aryan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->clip) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\aryan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->clip) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\aryan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->clip) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\aryan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->clip) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aryan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->clip) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\aryan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->clip) (2023.10.0)\n",
      "Requirement already satisfied: numpy<2 in c:\\users\\aryan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision->clip) (1.26.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\aryan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision->clip) (10.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aryan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->clip) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aryan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch->clip) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\aryan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch->clip) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "dataset_dir = \"D:/clip-embedding/processed_samples\"\n",
    "\n",
    "def load_images(dataset_dir):\n",
    "    images = []\n",
    "    image_paths = []\n",
    "    for filename in os.listdir(dataset_dir):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            image_path = os.path.join(dataset_dir, filename)\n",
    "            image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "            images.append(image)\n",
    "            image_paths.append(image_path)\n",
    "    return images, image_paths\n",
    "\n",
    "images, image_paths = load_images(dataset_dir)\n",
    "\n",
    "with torch.no_grad():\n",
    "    dataset_embeddings = torch.cat([model.encode_image(image) for image in images])\n",
    "    dataset_embeddings /= dataset_embeddings.norm(dim=-1, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar images:\n",
      "D:/clip-embedding/processed_samples\\Screenshot 2024-07-28 012028.png\n",
      "D:/clip-embedding/processed_samples\\output_image1.png\n",
      "D:/clip-embedding/processed_samples\\Screenshot 2024-07-28 014514.png\n",
      "D:/clip-embedding/processed_samples\\Screenshot 2024-07-28 013849.png\n",
      "D:/clip-embedding/processed_samples\\Screenshot 2024-07-28 013530.png\n"
     ]
    }
   ],
   "source": [
    "def find_similar_images(input_image_path, dataset_embeddings, image_paths, top_k=5):\n",
    "\n",
    "    input_image = preprocess(Image.open(input_image_path)).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_embedding = model.encode_image(input_image)\n",
    "        input_embedding /= input_embedding.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    similarities = (input_embedding @ dataset_embeddings.T).squeeze(0)\n",
    "\n",
    "    top_k_indices = similarities.topk(top_k).indices\n",
    "\n",
    "    similar_image_paths = [image_paths[idx] for idx in top_k_indices]\n",
    "\n",
    "    return similar_image_paths\n",
    "\n",
    "input_image_path = r\"D:/clip-embedding/processed_inputs/OOTD1.png\"\n",
    "similar_images = find_similar_images(input_image_path, dataset_embeddings, image_paths)\n",
    "\n",
    "print(\"Most similar images:\")\n",
    "for img_path in similar_images:\n",
    "    print(img_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
